{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION:\n",
    "    THIS IS A BASIC WEB CRAWLER BUILT TO CRAWL THROUGH PEC.AC.IN\n",
    "    IT GOES TO A PAGE, GETS ALL THE LINKS AVAILABLE ON THAT PAGE WITHIN THE <A HREF=\"\"> SECTION, AND STORES IT INTO A LIST\n",
    "    THIS SAME LIST IS USED AS REFFERENCE TO VISIT THE LISTED PAGES AND PERFORM THE SAME OPERATIONS ON THEM, THEREBY UPDATING THE URLs LIST\n",
    "    IT ALSO STORES THE TEXT OF EVERY PAGE IT VISITS.\n",
    "    ALL THIS DATA, i.e. THE LINK, THE CORRESPONDING LISTS ON THE PAGE, AND IT'S TEXT, ALL ARE STORED IN A CSV FILE.\n",
    "    \n",
    "USES:\n",
    "    USED FOR CHECKING DEAD LINKS, ERRORS IN PAGES/URLs AND A BIRD'S VIEW OVERLAY OF PEC.AC.IN\n",
    "    \n",
    "PROBELMS:\n",
    "    CAN'T STOP AT A CERTAIN LEVEL; CRAWLS TILL IT COVERS ALL SITES LISTED WITHIN PEC.AC.IN\n",
    "    DEPTH PERCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import OrderedDict \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_site = 'https://pec.ac.in'\n",
    "\n",
    "URL_Dict = OrderedDict() # Stores 'working_link':'all_links'\n",
    "\n",
    "URL_Text = OrderedDict() # Stores 'working_link':'text_on_page'\n",
    "\n",
    "URL_List = [main_site]  # We iterate over this list\n",
    "URL_Depth.update({main_site:1}) # Initialises mainpage to Depth = 1\n",
    "\n",
    "URL_Depth = OrderedDict() # Stores depth of each page; updates to shortest\n",
    "\n",
    "df = pd.DataFrame(columns=('Current_URL', 'Depth', 'URLs', 'Text_stuff')) # This is where we store everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to Go! https://pec.ac.in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaspreet/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to Go! https://pec.ac.in#main-content\n",
      "Good to Go! http://alumni.pec.ac.in/\n",
      "Good to Go! https://mail.pec.ac.in\n",
      "Good to Go! https://pec.ac.in/jobs\n",
      "Good to Go! https://pec.ac.in/tenders\n",
      "Good to Go! https://pec.ac.in/node/549\n",
      "Good to Go! http://pec.ac.in/tnp/about-cell\n",
      "Good to Go! https://pec.ac.in/\n",
      "Good to Go! https://pec.ac.in#\n",
      "Good to Go! https://pec.ac.in/institute\n",
      "Good to Go! https://pec.ac.in/history\n",
      "Good to Go! https://pec.ac.in/vision\n",
      "Good to Go! https://pec.ac.in/campus\n",
      "Good to Go! https://pec.ac.in/location\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for working_url in URL_List:\n",
    "    \n",
    "    if count == 15:\n",
    "        break\n",
    "    response = requests.get(working_url)\n",
    "\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print('ERROR!!! Contact Jaspreet!!!')\n",
    "        print(working_url)\n",
    "        continue\n",
    "    else:\n",
    "        print('Good to Go!', end = ' ')\n",
    "        print(working_url)\n",
    "\n",
    "        \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  \n",
    "\n",
    "    if URL_Dict.get(working_url, True): \n",
    "        \n",
    "        \n",
    "        \n",
    "        # add text\n",
    "        text_part = soup.get_text()\n",
    "        URL_Text.update({working_url:text_part})\n",
    "        \n",
    "        # make url list to add in dict\n",
    "        url_list = []\n",
    "        \n",
    "        # for every html tag, do\n",
    "        for i in soup.find_all(\"a\", href=True):\n",
    "            ans_url = i['href']\n",
    "            \n",
    "            # append pec.ac.in if not in url\n",
    "            if 'http' not in ans_url:\n",
    "                ans_url = working_url + ans_url   \n",
    "            # remove if it goes out of pec\n",
    "            if 'pec.ac' not in ans_url:\n",
    "                continue\n",
    "            \n",
    "            # update url list to upload\n",
    "            url_list.append(ans_url)\n",
    "            \n",
    "            # if url not iterated before, add it to main queue\n",
    "            if ans_url not in URL_List:\n",
    "                URL_List.append(ans_url)\n",
    "                \n",
    "                \n",
    "            # putting Depth\n",
    "            if URL_Depth.get(ans_url, True):\n",
    "                URL_Depth.update({ans_url: int(1 + URL_Depth[working_url])})\n",
    "            else:\n",
    "                orig_depth = URL_Depth.get(ans_url)\n",
    "                new_depth = int(1 + URL_Depth[working_ur])\n",
    "                if orig_depth>new_depth:\n",
    "                    URL_Depth.update({ans_url: new_depth})\n",
    "                else:\n",
    "                    URL_Depth.update({ans_url: orig_depth})\n",
    "        \n",
    "        # add to dictionary {'current_url':'all_urls_on_page'}\n",
    "        URL_Dict.update({working_url:url_list})\n",
    "        \n",
    "        csv_dict = [{'Current_URL':working_url, 'Depth':URL_Depth.get(working_url), 'URLs':url_list, 'Text_stuff':text_part}]\n",
    "        temp_df_row = pd.DataFrame(csv_dict)\n",
    "        \n",
    "        df = df.append(temp_df_row, ignore_index = True)\n",
    "        df.to_csv('PEC_Crawled_Data.csv')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = pd.DataFrame(columns=('lib', 'qty1', 'qty2'))\n",
    "\n",
    "#app1 = [{'lib':'heffffllo', 'qty1':'it', 'qty2':'works'}]\n",
    "\n",
    "#res = pd.DataFrame(app)\n",
    "#res = res.append(res1, ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
