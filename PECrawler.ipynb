{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION:\n",
    "    THIS IS A BASIC WEB CRAWLER BUILT TO CRAWL THROUGH PEC.AC.IN \n",
    "    IT GOES TO A PAGE, GETS ALL THE LINKS AVAILABLE ON THAT PAGE WITHIN THE <A HREF=\"\"> SECTION, AND STORES IT INTO A LIST \n",
    "    THIS SAME LIST IS USED AS REFFERENCE TO VISIT THE LISTED PAGES AND PERFORM THE SAME OPERATIONS ON THEM, THEREBY UPDATING THE URLs LIST\n",
    "    IT ALSO STORES THE TEXT OF EVERY PAGE IT VISITS.\n",
    "    A SPECIAL FEATURE INCLUDES SEARCHING BY KEYWORDS.\n",
    "    THE DEPTH OF EACH PAGE RELATIVE TO THE HOME PAGE IS SPECFIED\n",
    "    ALL THIS DATA, i.e. THE LINK, THE CORRESPONDING LISTS ON THE PAGE, IT'S TEXT, ETC. ALL ARE STORED IN A CSV FILE.\n",
    "    \n",
    "USES:\n",
    "    USED FOR CHECKING DEAD LINKS, ERRORS IN PAGES/URLs AND A BIRD'S VIEW OVERLAY OF PEC.AC.IN, SEARCHING SPECIFIC KEYWORDS, DEPTH DETECTION, ETC. CREATIVITY OF THE USER AND USES ARE PROPORTIONAL\n",
    "    \n",
    "PROBELMS:\n",
    "    PERFORMS UPTO EXPECTATIONS AS OF NOW; MAY FAIL WHEN NEWER REQUIREMENTS ARE MADE KNOW IN THE FUTURE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import OrderedDict \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alumini\n"
     ]
    }
   ],
   "source": [
    "main_site = 'https://pec.ac.in'\n",
    "\n",
    "URL_Dict = OrderedDict() # Stores 'working_link':'all_links'\n",
    "\n",
    "URL_Text = OrderedDict() # Stores 'working_link':'text_on_page'\n",
    "\n",
    "URL_List = [main_site]  # We iterate over this list\n",
    "\n",
    "URL_Depth = OrderedDict() # Stores depth of each page; updates to shortest\n",
    "URL_Depth.update({main_site:1}) # Initialises mainpage to Depth = 1\n",
    "\n",
    "df = pd.DataFrame(columns=('Current_URL', 'Depth', 'URLs', 'Text_stuff', 'Keyword')) # This is where we store everything\n",
    "\n",
    "key_word = str(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to Go! https://pec.ac.in\n",
      "Good to Go! https://pec.ac.in#main-content\n",
      "Good to Go! http://alumni.pec.ac.in/\n",
      "Good to Go! https://mail.pec.ac.in\n",
      "Good to Go! https://pec.ac.in/jobs\n",
      "Good to Go! https://pec.ac.in/tenders\n",
      "Good to Go! https://pec.ac.in/node/549\n",
      "Good to Go! http://pec.ac.in/tnp/about-cell\n",
      "Good to Go! https://pec.ac.in/\n",
      "Good to Go! https://pec.ac.in#\n",
      "Good to Go! https://pec.ac.in/institute\n",
      "Good to Go! https://pec.ac.in/history\n",
      "Good to Go! https://pec.ac.in/vision\n",
      "Good to Go! https://pec.ac.in/campus\n",
      "Good to Go! https://pec.ac.in/location\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for working_url in URL_List:\n",
    "    \n",
    "    if count == 15:\n",
    "        break\n",
    "    response = requests.get(working_url)\n",
    "\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print('ERROR!!! Contact Jaspreet!!!')\n",
    "        print(working_url)\n",
    "        continue\n",
    "    else:\n",
    "        print('Good to Go!', end = ' ')\n",
    "        print(working_url)\n",
    "\n",
    "        \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  \n",
    "\n",
    "    if URL_Dict.get(working_url, True): \n",
    "             \n",
    "        # add text\n",
    "        text_part = str(soup.get_text())\n",
    "        URL_Text.update({working_url:text_part})\n",
    "        \n",
    "        #check keyword\n",
    "        match = False        \n",
    "        if key_word in text_part:\n",
    "            match = True\n",
    "            \n",
    "        \n",
    "        # make url list to add in dict\n",
    "        url_list = []\n",
    "        \n",
    "        # for every html tag, do\n",
    "        for i in soup.find_all(\"a\", href=True):\n",
    "            ans_url = i['href']\n",
    "            \n",
    "            # append pec.ac.in if not in url\n",
    "            if 'http' not in ans_url:\n",
    "                ans_url = working_url + ans_url   \n",
    "            # remove if it goes out of pec\n",
    "            if 'pec.ac' not in ans_url:\n",
    "                continue\n",
    "            \n",
    "            # update url list to upload\n",
    "            url_list.append(ans_url)\n",
    "            \n",
    "            # if url not iterated before, add it to main queue\n",
    "            if ans_url not in URL_List:\n",
    "                URL_List.append(ans_url)\n",
    "                \n",
    "                \n",
    "            # putting Depth\n",
    "            URL_Depth.update({ans_url: int(1 + URL_Depth[working_url])})\n",
    "        \n",
    "        # add to dictionary {'current_url':'all_urls_on_page'}\n",
    "        URL_Dict.update({working_url:url_list})\n",
    "        \n",
    "        #creating an entry to add to CSV file\n",
    "        csv_dict = [{'Current_URL':working_url, 'Depth':URL_Depth.get(working_url), 'URLs':url_list, 'Text_stuff':text_part, 'Keyword':match}]\n",
    "        temp_df_entry = pd.DataFrame(csv_dict)\n",
    "        \n",
    "        #pushing entry into CSV file\n",
    "        df = df.append(temp_df_entry, ignore_index = True)\n",
    "        df.to_csv('Top_15_Crawler_Hits.csv')\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
